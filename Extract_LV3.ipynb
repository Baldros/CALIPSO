{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e65da5f-aaac-47c7-aa05-5ee0459e6ff1",
   "metadata": {},
   "source": [
    "# Apresentação:\n",
    "\n",
    "    **Escrever a apresentação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f14cd9c-6e44-4606-939e-7580d1b69837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pyhdf.SD import SD, SDC\n",
    "from processamento_dados import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae1dd9-3269-4679-a835-7e166845d2d8",
   "metadata": {},
   "source": [
    "# Extraindo os dados:\n",
    "\n",
    "    Temos dois códigos de LV3, a primeira versão é mais arcaico e a segundo versão é mais\n",
    "    bem elaborado. A principio, vamos tentar extrair as tabelas utilizadas do LV3 mais bem\n",
    "    elaborado e torcer para ele vir bem organizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a366d770-a9b3-4c15-8160-3f63d4dde011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defindo os diretórios:\n",
    "dir1 = r'D:\\Estudo\\LASA\\Dados\\Bahia\\L2_VFM'\n",
    "dir2 = r'D:\\Estudo\\LASA\\Dados\\Bahia\\L3_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea2b4a5-0631-4720-b7c9-24e68a868a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Data de Inicio(Ano):  \n",
      "Data de Final(Ano):  \n"
     ]
    }
   ],
   "source": [
    "# Inervalo de Tempo:\n",
    "ano_inicial = input('Data de Inicio(Ano): ')\n",
    "ano_final = input('Data de Final(Ano): ')\n",
    "\n",
    "    \n",
    "if ano_inicial == '':\n",
    "    ano_inicial = '2006'\n",
    "    \n",
    "elif int(ano_inicial) < 2006:\n",
    "    print('Não existe valores anteriores a 2006')\n",
    "\n",
    "    \n",
    "if ano_final == '':\n",
    "    ano_final = '2022'\n",
    "    \n",
    "elif int(ano_final) > 2022:\n",
    "    print('Não existe valores maiores que 2022')\n",
    "\n",
    "files = hdf_path(dir2, [ano_inicial,ano_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2782240c-9105-41de-b695-011e602fb6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando informações:\n",
    "info = json_reader('bahia_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c43c728-e209-4004-b14d-7e512d9cf816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n",
      "C:\\Users\\amori\\JupyterNotebook\\OneDrive\\LASA\\Experimentos\\processamento_dados.py:152: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(np.nanmean(dado, axis=0), axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Variável contendo informações do produto de perfil de aerossóis troposféricos\n",
    "info_produto = info['Produtos']['Tropospheric_Aerosol_Profile']\n",
    "\n",
    "#files = [f.path for f in os.scandir(info_produto['Diretório'])]\n",
    "\n",
    "niveis_altitude = SD(files[0], SDC.READ).select('Altitude_Midpoint').get()[0]\n",
    "perfil_media_sazonal_anual = {}\n",
    "perfil_media_sazonal = {}\n",
    "dados = {}\n",
    "for area, info_area in list(info['Áreas_estudo'].items()):\n",
    "    # Cria um dicionário vazio para cada área\n",
    "    perfil_media_sazonal_anual[area] = {}\n",
    "    perfil_media_sazonal[area] = {}\n",
    "    dados[area] = {}\n",
    "    for variavel, info_variavel in info_produto[\"Variáveis\"].items():\n",
    "        # Cria uma chave no dicionário para cada variável\n",
    "        dados[area][variavel] = {}\n",
    "        coordenadas = info_area['Coordenadas']\n",
    "        for file in files:\n",
    "            dado = SD(file, SDC.READ)\n",
    "            # Abre cada arquivo listado\n",
    "            # Seleciona dados de cada variável fazendo corte de área\n",
    "            dado = selecao_area(dado, coordenadas, variavel)\n",
    "            dado = controle_qualidade(dado, info_variavel['Limites_detecção'])\n",
    "            # Adicionar mês ao dicionário apenas quando 80% dos dados passam no CQ\n",
    "            if porcentagem_valida(dado) >= 0.8:\n",
    "                dados[area][variavel][file[-12:-5]] = calc_media_espacial(dado)\n",
    "\n",
    "        if variavel == 'Aerosol_Type':\n",
    "            # Transforma array contendo classificação dos aerossóis em dataframe com colunas correspondendo a cada tipo de aerossol\n",
    "            dados[area][variavel] = convert_dicionario_2d_dataframe(dados[area][variavel], info_variavel)\n",
    "            # Salvando df com médias sazonais para cada ano e tipo de aerossol na chave da variável de detecção de aerossóis por tipo\n",
    "            perfil_media_sazonal_anual[area][variavel] = calc_media_sazonal_anual_2d(dados[area][variavel])\n",
    "            perfil_media_sazonal[area][variavel] = calc_media_sazonal_2d(perfil_media_sazonal_anual[area][variavel])\n",
    "        else:\n",
    "            # Salvando df com as médias sazonais para cada ano de cada variável de perfil único na chave da variável\n",
    "            perfil_media_sazonal_anual[area][variavel] = calc_media_sazonal_anual_1d(dados[area][variavel])\n",
    "            # Salvando df com as médias sazonais do período para cada variável de perfil único na chave da variável\n",
    "            perfil_media_sazonal[area][variavel] = calc_media_sazonal_1d(perfil_media_sazonal_anual[area][variavel])\n",
    "        \n",
    "        if 'Ext' not in variavel:\n",
    "            # Calcula a soma das detecções para a coluna\n",
    "            dados[area][variavel] = calc_soma_deteccoes_coluna(dados[area][variavel])\n",
    "            # Tranformando dicionário em df\n",
    "            dados[area][variavel] = convert_dicionario_1d_dataframe(dados[area][variavel])\n",
    "            dados[area][variavel] = preenchendo_dados(dados[area][variavel])\n",
    "\n",
    "    dados[area]['AOD'] = pd.concat({variavel: valor for variavel, valor in dados[area].items() if 'AOD' in variavel}, axis=1).droplevel(1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340636e6-f632-43de-b5e5-b99243027076",
   "metadata": {},
   "source": [
    "# Transferência dos dados:\n",
    "\n",
    "    Algumas tabelas estão com a data como index. Não queremos isso dessa forma, porque\n",
    "    na hora de passar para o SGBD pode ser que percamos essa informação. Então vamos\n",
    "    tratar esses dados para extrair essa informação da melhor forma possivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "612ab832-66b5-4222-993c-4d11a14a9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from conection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80be163-14a2-492c-8427-f47fc2fddcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectado ao SGBD\n"
     ]
    }
   ],
   "source": [
    "# Conectando-se ao SGBD:\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"bd_project\",\n",
    "  password=\"*****\"\n",
    ")\n",
    "\n",
    "if mydb.is_connected() == True:\n",
    "    print('Conectado ao SGBD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "855eb90b-a229-42ec-acd6-7cad5c887475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o cursor para transferência de informação:\n",
    "cursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12dc228d-fbcb-45b1-8a05-e5bb9974a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:23<00:00,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo Concluido:\n",
      "Tempo de processamento: 23 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando Schema:\n",
    "inicio = time()\n",
    "for area in tqdm(dados.keys()):\n",
    "    cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {area}\")\n",
    "    cursor.execute(f\"USE {area}\")\n",
    "\n",
    "    # Criando tabelas de Coeficiente de Extinção:\n",
    "    for key in dados[area].keys():\n",
    "        if 'Extinction_Coefficient' in key:\n",
    "            colunas = ', '.join([f'`{col}` FLOAT' for col in dados[area][key]])\n",
    "            cursor.execute(\n",
    "                f'''\n",
    "                CREATE TABLE IF NOT EXISTS {key} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    {colunas}\n",
    "                );\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            # Transferindo dados:\n",
    "            dataframe = pd.DataFrame(dados[area][key])\n",
    "            # Ajustando valores nan para serem inseridos na tabela:\n",
    "            dataframe.replace({'nan': None, np.nan: None}, inplace=True)\n",
    "        \n",
    "            # Extraindo as colunas como string:\n",
    "            colunas = ', '.join([f'`{col}`' for col in dataframe.columns])\n",
    "        \n",
    "        \n",
    "            # Preparando os valores para inserção\n",
    "            values = [tuple(row) for row in dataframe.to_numpy()]\n",
    "        \n",
    "            # Construindo a instrução SQL com %s para cada valor\n",
    "            insert = f\"INSERT INTO {key} ({colunas}) VALUES ({', '.join(['%s'] * len(dataframe.columns))})\"\n",
    "        \n",
    "            # Executando a inserção em lote\n",
    "            cursor.executemany(insert, values)\n",
    "            mydb.commit()\n",
    "        \n",
    "        elif key == \"Aerosol_Type\":\n",
    "            # Construindo Tabelas:\n",
    "            cursor.execute(\n",
    "                f'''\n",
    "                CREATE TABLE IF NOT EXISTS {key} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    Data VARCHAR(20),\n",
    "                    Marinho_Limpo FLOAT,\n",
    "                    Poeira FLOAT,\n",
    "                    Poluicao_Continental_Fumaca FLOAT,\n",
    "                    Continental_Limpo FLOAT,\n",
    "                    Poeira_Poluida FLOAT,\n",
    "                    Fumaca_Elevada FLOAT,\n",
    "                    Poeira_Marinha FLOAT\n",
    "                );\n",
    "                '''\n",
    "            )\n",
    "    \n",
    "            # Inserindo dados:\n",
    "            dataframe = pd.DataFrame(dados[area][key]).reset_index().rename(columns={'index': 'Data'})\n",
    "            dataframe.replace({np.nan: None}, inplace=True)\n",
    "            \n",
    "            for index, row in dataframe.iterrows():\n",
    "                insert = '''INSERT INTO Aerosol_Type (Data, Marinho_Limpo, Poeira,\n",
    "                Poluicao_Continental_Fumaca,Continental_Limpo,Poeira_Poluida,Fumaca_Elevada,Poeira_Marinha)\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s)'''\n",
    "                val = (row['Data'],row['Marinho Limpo'],row['Poeira'],\n",
    "                       row['Poluição Continental/Fumaça'],row['Continental Limpo'],row['Poeira Poluída'],\n",
    "                       row['Fumaça Elevada'],row['Poeira Marinha'])\n",
    "                cursor.execute(insert, val)\n",
    "                mydb.commit()\n",
    "\n",
    "        elif key == \"AOD\":\n",
    "            # Construindo Tabelas:\n",
    "            cursor.execute(\n",
    "                f'''\n",
    "                CREATE TABLE IF NOT EXISTS {key} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    Data VARCHAR(20),\n",
    "                    AOD_Mean FLOAT,\n",
    "                    AOD_Mean_Dust FLOAT,\n",
    "                    AOD_Mean_Elevated_Smoke FLOAT,\n",
    "                    AOD_Mean_Polluted_Dust FLOAT\n",
    "                );\n",
    "                '''\n",
    "            )\n",
    "    \n",
    "            # Inserindo dados:\n",
    "            dataframe = pd.DataFrame(dados[area][key]).reset_index().rename(columns={'index': 'Data'})\n",
    "            dataframe.replace({np.nan: None}, inplace=True)\n",
    "    \n",
    "            for index, row in dataframe.iterrows():\n",
    "                insert = '''INSERT INTO AOD (Data, AOD_Mean, AOD_Mean_Dust,AOD_Mean_Elevated_Smoke,AOD_Mean_Polluted_Dust)\n",
    "                VALUES (%s,%s,%s,%s,%s)'''\n",
    "                val = (row['Data'],row['AOD_Mean'],row['AOD_Mean_Dust'],\n",
    "                       row['AOD_Mean_Elevated_Smoke'],row['AOD_Mean_Polluted_Dust'])\n",
    "                cursor.execute(insert, val)\n",
    "                mydb.commit()\n",
    "    \n",
    "        \n",
    "        else:\n",
    "            cursor.execute(\n",
    "                f'''\n",
    "                CREATE TABLE IF NOT EXISTS {key} (\n",
    "                Data VARCHAR(20),\n",
    "                Valores FLOAT\n",
    "                )\n",
    "                '''\n",
    "            )\n",
    "\n",
    "            # Transferindo dados:\n",
    "            dataframe = pd.DataFrame(dados[area][key])\n",
    "            dataframe.rename(columns={0:'Valores'},inplace=True)\n",
    "            # Ajustando valores nan para serem inseridos na tabela:\n",
    "            dataframe.replace({'nan': None, np.nan: None}, inplace=True)\n",
    "        \n",
    "            # Extraindo as colunas como string:\n",
    "            colunas = ', '.join([f'`{col}`' for col in dataframe.columns])\n",
    "        \n",
    "        \n",
    "            # Preparando os valores para inserção\n",
    "            values = [tuple(row) for row in dataframe.to_numpy()]\n",
    "        \n",
    "            # Construindo a instrução SQL com %s para cada valor\n",
    "            insert = f\"INSERT INTO {key} ({colunas}) VALUES ({', '.join(['%s'] * len(dataframe.columns))})\"\n",
    "        \n",
    "            # Executando a inserção em lote\n",
    "            cursor.executemany(insert, values)\n",
    "            mydb.commit()\n",
    "\n",
    "\n",
    "            \n",
    "final = time()\n",
    "print('Processo Concluido:')\n",
    "print('Tempo de processamento:', int(final - inicio), 'segundos')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
